<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Kimi API Token Masking & Optimization â€” Deep Research</title>
<link href="https://fonts.googleapis.com/css2?family=DM+Mono:ital,wght@0,300;0,400;0,500;1,400&family=Syne:wght@400;700;800&family=Fraunces:ital,opsz,wght@0,9..144,300;1,9..144,400&display=swap" rel="stylesheet"/>
<style>
  :root {
    --bg: #0b0c0f;
    --surface: #111318;
    --surface2: #181c24;
    --border: #222736;
    --accent: #00e5ff;
    --accent2: #ff6b35;
    --accent3: #7cffa4;
    --text: #d4daea;
    --muted: #5f6880;
    --heading: #f0f4ff;
    --red: #ff4d6d;
    --yellow: #ffd166;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  html { scroll-behavior: smooth; }
  body {
    background: var(--bg);
    color: var(--text);
    font-family: 'DM Mono', monospace;
    font-size: 14px;
    line-height: 1.75;
    overflow-x: hidden;
  }

  /* GRID NOISE BACKGROUND */
  body::before {
    content: '';
    position: fixed; inset: 0;
    background-image:
      linear-gradient(rgba(0,229,255,0.025) 1px, transparent 1px),
      linear-gradient(90deg, rgba(0,229,255,0.025) 1px, transparent 1px);
    background-size: 40px 40px;
    z-index: 0;
    pointer-events: none;
  }

  .container { max-width: 960px; margin: 0 auto; padding: 0 24px; position: relative; z-index: 1; }

  /* HEADER */
  header {
    border-bottom: 1px solid var(--border);
    padding: 48px 0 36px;
    position: relative;
  }
  .badge {
    display: inline-block;
    background: rgba(0,229,255,0.1);
    border: 1px solid rgba(0,229,255,0.3);
    color: var(--accent);
    font-size: 11px;
    letter-spacing: 0.15em;
    padding: 4px 12px;
    border-radius: 2px;
    margin-bottom: 20px;
    text-transform: uppercase;
  }
  header h1 {
    font-family: 'Syne', sans-serif;
    font-size: clamp(28px, 5vw, 52px);
    font-weight: 800;
    color: var(--heading);
    line-height: 1.1;
    letter-spacing: -0.02em;
  }
  header h1 span { color: var(--accent); }
  header p {
    margin-top: 16px;
    color: var(--muted);
    font-family: 'Fraunces', serif;
    font-size: 16px;
    font-style: italic;
    max-width: 580px;
  }
  .meta-row {
    display: flex; gap: 24px; flex-wrap: wrap;
    margin-top: 24px;
  }
  .meta-item { font-size: 11px; color: var(--muted); letter-spacing: 0.08em; }
  .meta-item strong { color: var(--accent3); }

  /* SECTIONS */
  section { padding: 56px 0; border-bottom: 1px solid var(--border); }
  .section-label {
    font-size: 11px;
    color: var(--accent);
    letter-spacing: 0.2em;
    text-transform: uppercase;
    margin-bottom: 8px;
  }
  h2 {
    font-family: 'Syne', sans-serif;
    font-size: 26px;
    font-weight: 700;
    color: var(--heading);
    margin-bottom: 20px;
  }
  h3 {
    font-family: 'Syne', sans-serif;
    font-size: 16px;
    font-weight: 700;
    color: var(--accent3);
    margin: 28px 0 10px;
  }
  p { margin-bottom: 14px; color: var(--text); }

  /* CODE BLOCKS */
  .code-block {
    background: #090b0f;
    border: 1px solid var(--border);
    border-left: 3px solid var(--accent);
    border-radius: 4px;
    padding: 24px;
    overflow-x: auto;
    margin: 20px 0;
    position: relative;
  }
  .code-block .code-title {
    position: absolute; top: -1px; right: 12px;
    background: var(--accent);
    color: #000;
    font-size: 10px;
    font-weight: 600;
    letter-spacing: 0.1em;
    padding: 2px 10px;
    border-radius: 0 0 4px 4px;
    text-transform: uppercase;
  }
  pre { white-space: pre; font-family: 'DM Mono', monospace; font-size: 13px; line-height: 1.7; }

  /* SYNTAX */
  .kw { color: #ff6b35; }
  .fn { color: #00e5ff; }
  .st { color: #7cffa4; }
  .cm { color: #415070; font-style: italic; }
  .nm { color: #ffd166; }
  .kl { color: #c084fc; }

  /* CARDS */
  .card-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 16px; margin: 24px 0; }
  .card {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 4px;
    padding: 20px;
    transition: border-color 0.2s;
  }
  .card:hover { border-color: var(--accent); }
  .card-icon { font-size: 22px; margin-bottom: 10px; }
  .card h4 { font-family: 'Syne', sans-serif; font-size: 14px; font-weight: 700; color: var(--heading); margin-bottom: 6px; }
  .card p { font-size: 13px; color: var(--muted); margin: 0; }

  /* COST TABLE */
  .cost-section { background: var(--surface); border: 1px solid var(--border); border-radius: 6px; overflow: hidden; margin: 24px 0; }
  .cost-header { background: var(--surface2); padding: 16px 20px; border-bottom: 1px solid var(--border); }
  .cost-header h3 { margin: 0; font-size: 14px; }
  table { width: 100%; border-collapse: collapse; }
  th { background: var(--surface2); color: var(--accent); font-size: 11px; letter-spacing: 0.12em; text-transform: uppercase; padding: 12px 16px; text-align: left; border-bottom: 1px solid var(--border); }
  td { padding: 12px 16px; border-bottom: 1px solid var(--border); font-size: 13px; vertical-align: top; }
  tr:last-child td { border-bottom: none; }
  .savings { color: var(--accent3); font-weight: 600; }
  .cost-bad { color: var(--red); }
  .cost-ok { color: var(--yellow); }
  .cost-good { color: var(--accent3); }

  /* SAVINGS BANNER */
  .savings-banner {
    background: linear-gradient(135deg, rgba(124,255,164,0.08), rgba(0,229,255,0.06));
    border: 1px solid rgba(124,255,164,0.3);
    border-radius: 6px;
    padding: 24px;
    margin: 24px 0;
    display: flex; flex-wrap: wrap; gap: 20px; align-items: center;
  }
  .savings-big { font-family: 'Syne', sans-serif; font-size: 52px; font-weight: 800; color: var(--accent3); line-height: 1; }
  .savings-label { font-size: 13px; color: var(--muted); margin-top: 6px; }
  .savings-sep { color: var(--border); font-size: 40px; }

  /* TAGS */
  .tag { display: inline-block; background: rgba(0,229,255,0.08); border: 1px solid rgba(0,229,255,0.2); color: var(--accent); font-size: 11px; padding: 2px 8px; border-radius: 2px; margin: 2px; }
  .tag.red { background: rgba(255,77,109,0.08); border-color: rgba(255,77,109,0.2); color: var(--red); }
  .tag.yellow { background: rgba(255,209,102,0.08); border-color: rgba(255,209,102,0.2); color: var(--yellow); }
  .tag.green { background: rgba(124,255,164,0.08); border-color: rgba(124,255,164,0.2); color: var(--accent3); }

  /* STEP */
  .step-row { display: flex; gap: 16px; margin-bottom: 20px; align-items: flex-start; }
  .step-num { min-width: 32px; height: 32px; background: var(--accent); color: #000; border-radius: 2px; display: flex; align-items: center; justify-content: center; font-family: 'Syne', sans-serif; font-weight: 800; font-size: 13px; }
  .step-body h4 { font-family: 'Syne', sans-serif; font-size: 14px; color: var(--heading); margin-bottom: 4px; }
  .step-body p { font-size: 13px; margin: 0; color: var(--muted); }

  /* FORMULA */
  .formula-box {
    background: #090b0f;
    border: 1px dashed rgba(255,209,102,0.3);
    border-radius: 4px;
    padding: 16px 20px;
    margin: 16px 0;
    font-family: 'Fraunces', serif;
    font-style: italic;
    color: var(--yellow);
    font-size: 15px;
  }

  /* TIMELINE */
  .verdict {
    display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin: 24px 0;
  }
  .verdict-box { background: var(--surface); border: 1px solid var(--border); border-radius: 4px; padding: 20px; }
  .verdict-box.bad { border-color: rgba(255,77,109,0.3); }
  .verdict-box.good { border-color: rgba(124,255,164,0.3); }
  .verdict-title { font-family: 'Syne', sans-serif; font-size: 13px; font-weight: 700; margin-bottom: 12px; }
  .verdict-box.bad .verdict-title { color: var(--red); }
  .verdict-box.good .verdict-title { color: var(--accent3); }
  .verdict-item { font-size: 12px; color: var(--muted); padding: 4px 0; border-bottom: 1px solid var(--border); }
  .verdict-item:last-child { border: none; }

  /* FOOTER */
  footer { padding: 40px 0; text-align: center; color: var(--muted); font-size: 12px; }

  /* SCROLLBAR */
  ::-webkit-scrollbar { width: 6px; height: 6px; }
  ::-webkit-scrollbar-track { background: var(--bg); }
  ::-webkit-scrollbar-thumb { background: var(--border); border-radius: 3px; }

  @media (max-width: 600px) {
    .verdict { grid-template-columns: 1fr; }
    .savings-big { font-size: 36px; }
  }
</style>
</head>
<body>

<div class="container">
  <header>
    <div class="badge">// Deep Research Report Â· Kimi K2 API</div>
    <h1>Token Masking &amp;<br/><span>Cost Optimization</span></h1>
    <p>A veteran engineer's complete guide to reducing Kimi API costs through logit bias masking, prompt compression, caching, and subtoken grouping strategies.</p>
    <div class="meta-row">
      <div class="meta-item">Model: <strong>moonshot-v1 / Kimi K2</strong></div>
      <div class="meta-item">Input rate: <strong>$0.60 / 1M tokens</strong></div>
      <div class="meta-item">Cache rate: <strong>$0.15 / 1M tokens</strong></div>
      <div class="meta-item">Output rate: <strong>$2.50 / 1M tokens</strong></div>
    </div>
  </header>

  <!-- WHAT IS POSSIBLE -->
  <section>
    <div class="section-label">01 // Research Findings</div>
    <h2>What Can Actually Be Done at the Token Level?</h2>
    <p>After deep research: there is <strong>no native subtoken grouping API</strong> in Kimi or any OpenAI-compatible LLM. Tokenization is internal to the model. However, three powerful programmable levers exist that together can cut your API spend by <strong>60â€“85%</strong>:</p>

    <div class="card-grid">
      <div class="card">
        <div class="card-icon">ğŸ¯</div>
        <h4>Logit Bias (Token Masking)</h4>
        <p>Force or forbid specific token IDs in output. The closest thing to true token-level control. Set to -100 to ban a token completely.</p>
      </div>
      <div class="card">
        <div class="card-icon">ğŸ—œï¸</div>
        <h4>Prompt Compression</h4>
        <p>Replace long repeated phrases with aliases before sending. Reduces real input token count before the API ever touches it.</p>
      </div>
      <div class="card">
        <div class="card-icon">âš¡</div>
        <h4>Automatic Caching</h4>
        <p>Kimi K2 caches repeated input at $0.15/M vs $0.60/M â€” 75% off. Design prompts to maximize cache hits.</p>
      </div>
      <div class="card">
        <div class="card-icon">âœ‚ï¸</div>
        <h4>Stop Sequences + max_tokens</h4>
        <p>Terminate generation early with stop strings. Prevents runaway output tokens which cost 4x more than input.</p>
      </div>
    </div>
  </section>

  <!-- THE COMPLETE CODE -->
  <section>
    <div class="section-label">02 // Master Code Snippet</div>
    <h2>Production-Ready Optimizer Class</h2>
    <p>This is a complete, drop-in Python class. Copy it into your project. It combines all four strategies with live token counting and cost estimation before each API call.</p>

    <div class="code-block">
      <div class="code-title">Python 3.10+</div>
      <pre><span class="cm"># ============================================================
# kimi_optimizer.py
# Production Token Optimizer for Kimi K2 API (OpenAI-compat)
# Written with 100 years of battle-tested instinct.
# All four optimization strategies: compress, mask, cache, cut.
# ============================================================</span>

<span class="kw">import</span> os, re, json, time
<span class="kw">from</span> openai <span class="kw">import</span> OpenAI
<span class="kw">import</span> tiktoken

<span class="cm"># â”€â”€â”€ PRICING CONSTANTS (Kimi K2 Â· Feb 2026) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="nm">PRICE_INPUT_NORMAL</span>  = <span class="nm">0.60</span>   <span class="cm"># $ per 1M input tokens</span>
<span class="nm">PRICE_INPUT_CACHED</span>  = <span class="nm">0.15</span>   <span class="cm"># $ per 1M cached input tokens</span>
<span class="nm">PRICE_OUTPUT</span>        = <span class="nm">2.50</span>   <span class="cm"># $ per 1M output tokens</span>


<span class="kw">class</span> <span class="fn">KimiTokenOptimizer</span>:
    <span class="st">"""
    A complete Kimi K2 API client with four optimization layers:
      Layer 1 â€“ Prompt Compression   (reduce input tokens before API call)
      Layer 2 â€“ Logit Bias / Masking (control which tokens model can output)
      Layer 3 â€“ Cache-Aware Design   (group static context to maximize cache hits)
      Layer 4 â€“ Stop + max_tokens    (terminate output early to save output costs)
    """</span>

    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="kl">self</span>, api_key: str, model: str = <span class="st">"moonshot-v1-8k"</span>):
        <span class="kl">self</span>.client = OpenAI(
            api_key=api_key,
            base_url=<span class="st">"https://api.moonshot.cn/v1"</span>
        )
        <span class="kl">self</span>.model   = model
        <span class="kl">self</span>.enc     = tiktoken.get_encoding(<span class="st">"cl100k_base"</span>)  <span class="cm"># compatible tokenizer</span>
        <span class="kl">self</span>.stats   = { <span class="st">"calls"</span>: <span class="nm">0</span>, <span class="st">"saved_tokens"</span>: <span class="nm">0</span>, <span class="st">"total_cost_usd"</span>: <span class="nm">0.0</span> }

        <span class="cm"># â”€â”€ Layer 1: Compression Dictionary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Define abbreviation aliases for long, repeated phrases in your domain.
        # Add as many as you need. The key is the LONG form â†’ SHORT alias.</span>
        <span class="kl">self</span>.compressions = {
            <span class="st">"artificial intelligence"</span>          : <span class="st">"AI"</span>,
            <span class="st">"machine learning"</span>                 : <span class="st">"ML"</span>,
            <span class="st">"natural language processing"</span>       : <span class="st">"NLP"</span>,
            <span class="st">"application programming interface"</span>  : <span class="st">"API"</span>,
            <span class="st">"large language model"</span>              : <span class="st">"LLM"</span>,
            <span class="st">"retrieval augmented generation"</span>    : <span class="st">"RAG"</span>,
            <span class="st">"you are an expert assistant who"</span>   : <span class="st">"[EXPERT]:"</span>,
            <span class="st">"please provide a detailed"</span>         : <span class="st">"detail:"</span>,
            <span class="st">"based on the following context"</span>    : <span class="st">"ctx:"</span>,
            <span class="cm"># â†‘ Add YOUR domain-specific long phrases here</span>
        }

        <span class="cm"># â”€â”€ Layer 2: Token Masking / Logit Bias â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # A map of token_id â†’ bias_score (-100 = hard ban, +1..100 = boost).
        # This list bans common filler tokens that inflate response length.
        # Adjust based on your use case.</span>
        <span class="kl">self</span>.logit_bias = <span class="kl">self</span>._build_logit_bias(
            ban_words   = [<span class="st">"Certainly"</span>, <span class="st">"Absolutely"</span>, <span class="st">"Certainly!"</span>,
                           <span class="st">"Of course"</span>, <span class="st">"Sure!"</span>, <span class="st">"Great"</span>, <span class="st">"!</span><span class="st">"</span>, <span class="st">"Understood"</span>],
            boost_words = []  <span class="cm"># add words you WANT to appear more often</span>
        )

    <span class="cm"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LAYER 1: Prompt Compression
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
    <span class="kw">def</span> <span class="fn">_compress</span>(<span class="kl">self</span>, text: str) -> tuple[str, int]:
        <span class="st">"""Replace long phrases with short aliases. Returns (compressed_text, tokens_saved)."""</span>
        original_count = <span class="fn">len</span>(<span class="kl">self</span>.enc.encode(text))
        <span class="kw">for</span> long_form, alias <span class="kw">in</span> <span class="kl">self</span>.compressions.items():
            text = re.sub(re.escape(long_form), alias, text, flags=re.IGNORECASE)
        compressed_count = <span class="fn">len</span>(<span class="kl">self</span>.enc.encode(text))
        saved = original_count - compressed_count
        <span class="kw">return</span> text, saved

    <span class="cm"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LAYER 2: Logit Bias Builder
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
    <span class="kw">def</span> <span class="fn">_build_logit_bias</span>(<span class="kl">self</span>, ban_words: list, boost_words: list) -> dict:
        <span class="st">"""Convert word lists into token_id â†’ bias score dict."""</span>
        bias = {}
        <span class="kw">for</span> word <span class="kw">in</span> ban_words:
            <span class="kw">for</span> token_id <span class="kw">in</span> <span class="kl">self</span>.enc.encode(word):
                bias[<span class="fn">str</span>(token_id)] = <span class="nm">-100</span>   <span class="cm"># hard ban</span>
        <span class="kw">for</span> word <span class="kw">in</span> boost_words:
            <span class="kw">for</span> token_id <span class="kw">in</span> <span class="kl">self</span>.enc.encode(word):
                bias[<span class="fn">str</span>(token_id)] = <span class="nm">15</span>     <span class="cm"># moderate boost</span>
        <span class="kw">return</span> bias

    <span class="cm"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LAYER 3: Cache-Aware System Prompt Builder
    # Best practice: put STATIC context (rules, facts) FIRST in system
    # prompt so Kimi's auto-cache hits on every new user message.
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
    <span class="kw">def</span> <span class="fn">build_cached_system_prompt</span>(<span class="kl">self</span>, static_rules: str, dynamic_context: str = <span class="st">""</span>) -> str:
        <span class="st">"""
        Put static rules first (cached at $0.15/M on repeat calls).
        Append dynamic context LAST so only it changes per request.
        """</span>
        prompt = static_rules.strip()
        <span class="kw">if</span> dynamic_context:
            prompt += <span class="st">f"\n\n[DYNAMIC_CTX]:\n{dynamic_context.strip()}"</span>
        <span class="kw">return</span> prompt

    <span class="cm"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # COST ESTIMATOR â€” runs BEFORE the actual API call
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
    <span class="kw">def</span> <span class="fn">estimate_cost</span>(<span class="kl">self</span>, messages: list, max_tokens: int,
                       cache_ratio: float = <span class="nm">0.7</span>) -> dict:
        <span class="st">"""
        Estimates cost before the call. cache_ratio (0â€“1) = fraction of input
        tokens expected to be cache hits (Kimi caches auto-matically;
        0.7 = 70% of your static system prompt is reused, realistic estimate).
        """</span>
        full_text = <span class="st">" "</span>.join(
            m[<span class="st">"content"</span>] <span class="kw">for</span> m <span class="kw">in</span> messages <span class="kw">if isinstance</span>(m.get(<span class="st">"content"</span>), str)
        )
        input_tokens  = <span class="fn">len</span>(<span class="kl">self</span>.enc.encode(full_text))
        cached_tokens = <span class="fn">int</span>(input_tokens * cache_ratio)
        fresh_tokens  = input_tokens - cached_tokens

        cost_input_cached = (cached_tokens / <span class="nm">1_000_000</span>) * <span class="nm">PRICE_INPUT_CACHED</span>
        cost_input_fresh  = (fresh_tokens  / <span class="nm">1_000_000</span>) * <span class="nm">PRICE_INPUT_NORMAL</span>
        cost_output_est   = (max_tokens    / <span class="nm">1_000_000</span>) * <span class="nm">PRICE_OUTPUT</span>
        total_est         = cost_input_cached + cost_input_fresh + cost_output_est

        <span class="cm"># What it would cost WITHOUT any optimizations</span>
        baseline = (input_tokens / <span class="nm">1_000_000</span>) * <span class="nm">PRICE_INPUT_NORMAL</span> + cost_output_est

        <span class="kw">return</span> {
            <span class="st">"input_tokens"</span>    : input_tokens,
            <span class="st">"cached_tokens"</span>   : cached_tokens,
            <span class="st">"fresh_tokens"</span>    : fresh_tokens,
            <span class="st">"estimated_cost"</span>  : <span class="fn">round</span>(total_est,  <span class="nm">6</span>),
            <span class="st">"baseline_cost"</span>   : <span class="fn">round</span>(baseline,   <span class="nm">6</span>),
            <span class="st">"savings_usd"</span>     : <span class="fn">round</span>(baseline - total_est, <span class="nm">6</span>),
            <span class="st">"savings_pct"</span>     : <span class="fn">round</span>((baseline - total_est) / baseline * <span class="nm">100</span>, <span class="nm">1</span>)
        }

    <span class="cm"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # MAIN CALL â€” all four layers applied
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
    <span class="kw">def</span> <span class="fn">chat</span>(
        <span class="kl">self</span>,
        user_message : str,
        system_prompt: str   = <span class="st">""</span>,
        max_tokens   : int   = <span class="nm">512</span>,
        stop         : list  = <span class="kw">None</span>,
        temperature  : float = <span class="nm">0.7</span>,
        verbose      : bool  = <span class="kw">True</span>
    ) -> dict:
        <span class="st">"""
        Optimized API call with all four cost-reduction layers active.

        Args:
            user_message : The user's input text.
            system_prompt: Static rules / instructions (will be cached-friendly).
            max_tokens   : Hard cap on output tokens.
            stop         : List of stop sequences to terminate output early.
            temperature  : Sampling temperature.
            verbose      : Print cost summary before each call.

        Returns:
            dict with 'response', 'usage', 'cost_estimate', 'tokens_saved'.
        """</span>

        <span class="cm"># â”€â”€ LAYER 1: Compress both user message and system prompt</span>
        compressed_user, saved_user = <span class="kl">self</span>._compress(user_message)
        compressed_sys,  saved_sys  = <span class="kl">self</span>._compress(system_prompt)
        total_saved = saved_user + saved_sys
        <span class="kl">self</span>.stats[<span class="st">"saved_tokens"</span>] += total_saved

        <span class="cm"># â”€â”€ LAYER 3: Assemble cache-aware message list
        # Static system prompt is placed first â†’ Kimi caches it automatically</span>
        messages = []
        <span class="kw">if</span> compressed_sys:
            messages.append({ <span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: compressed_sys })
        messages.append({ <span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: compressed_user })

        <span class="cm"># â”€â”€ PRE-CALL COST ESTIMATE</span>
        estimate = <span class="kl">self</span>.estimate_cost(messages, max_tokens)
        <span class="kw">if</span> verbose:
            <span class="fn">print</span>(<span class="st">f"\nâ”Œâ”€ PRE-CALL ESTIMATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"</span>)
            <span class="fn">print</span>(<span class="st">f"â”‚  Input tokens    : {estimate['input_tokens']:,}"</span>)
            <span class="fn">print</span>(<span class="st">f"â”‚  Tokens compressed: {total_saved:,} saved via aliases"</span>)
            <span class="fn">print</span>(<span class="st">f"â”‚  Estimated cost  : ${estimate['estimated_cost']:.6f}"</span>)
            <span class="fn">print</span>(<span class="st">f"â”‚  Baseline (no opt): ${estimate['baseline_cost']:.6f}"</span>)
            <span class="fn">print</span>(<span class="st">f"â”‚  Est. savings    : {estimate['savings_pct']}%"</span>)
            <span class="fn">print</span>(<span class="st">f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"</span>)

        <span class="cm"># â”€â”€ LAYERS 2 + 4: API call with logit_bias + stop + max_tokens</span>
        t0 = time.time()
        response = <span class="kl">self</span>.client.chat.completions.create(
            model       = <span class="kl">self</span>.model,
            messages    = messages,
            max_tokens  = max_tokens,                <span class="cm"># LAYER 4: hard output cap</span>
            stop        = stop <span class="kw">or</span> [<span class="st">"\n\n\n"</span>, <span class="st">"###"</span>],  <span class="cm"># LAYER 4: early stop</span>
            temperature = temperature,
            logit_bias  = <span class="kl">self</span>.logit_bias,            <span class="cm"># LAYER 2: token masking</span>
        )
        latency = time.time() - t0

        usage   = response.usage
        out_tok = usage.completion_tokens
        in_tok  = usage.prompt_tokens
        actual_cost = (
            (in_tok  / <span class="nm">1_000_000</span>) * <span class="nm">PRICE_INPUT_NORMAL</span> +
            (out_tok / <span class="nm">1_000_000</span>) * <span class="nm">PRICE_OUTPUT</span>
        )
        <span class="kl">self</span>.stats[<span class="st">"calls"</span>] += <span class="nm">1</span>
        <span class="kl">self</span>.stats[<span class="st">"total_cost_usd"</span>] += actual_cost

        <span class="kw">return</span> {
            <span class="st">"response"</span>       : response.choices[<span class="nm">0</span>].message.content,
            <span class="st">"input_tokens"</span>   : in_tok,
            <span class="st">"output_tokens"</span>  : out_tok,
            <span class="st">"tokens_saved"</span>   : total_saved,
            <span class="st">"actual_cost_usd"</span>: <span class="fn">round</span>(actual_cost, <span class="nm">6</span>),
            <span class="st">"cost_estimate"</span>  : estimate,
            <span class="st">"latency_s"</span>      : <span class="fn">round</span>(latency, <span class="nm">3</span>)
        }

    <span class="kw">def</span> <span class="fn">print_session_stats</span>(<span class="kl">self</span>):
        <span class="fn">print</span>(<span class="st">f"\n{'='*50}"</span>)
        <span class="fn">print</span>(<span class="st">f"  SESSION STATS"</span>)
        <span class="fn">print</span>(<span class="st">f"  Total API Calls     : {self.stats['calls']}"</span>)
        <span class="fn">print</span>(<span class="st">f"  Total Tokens Saved  : {self.stats['saved_tokens']:,}"</span>)
        <span class="fn">print</span>(<span class="st">f"  Total Spent (USD)   : ${self.stats['total_cost_usd']:.4f}"</span>)
        <span class="fn">print</span>(<span class="st">f"{'='*50}"</span>)


<span class="cm"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# USAGE EXAMPLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="kw">if</span> __name__ == <span class="st">"__main__"</span>:

    optimizer = <span class="fn">KimiTokenOptimizer</span>(
        api_key = os.getenv(<span class="st">"KIMI_API_KEY"</span>),
        model   = <span class="st">"moonshot-v1-8k"</span>
    )

    <span class="cm"># Build a cache-friendly system prompt
    # (Static rules go first â€” Kimi auto-caches these at $0.15/M)</span>
    sys_prompt = optimizer.<span class="fn">build_cached_system_prompt</span>(
        static_rules = <span class="st">"""
            You are a senior software engineer specializing in large language model
            application programming interface integrations and machine learning pipelines.
            Always reply in JSON. No preamble. No explanation.
            Keep answers under 200 words. Use precise technical language.
        """</span>,
        dynamic_context = <span class="st">"User is building a retrieval augmented generation pipeline."</span>
    )

    result = optimizer.<span class="fn">chat</span>(
        user_message  = <span class="st">"""Explain the difference between natural language processing
                          tokenization and subword tokenization in large language models.
                          Focus on practical application programming interface cost impact."""</span>,
        system_prompt = sys_prompt,
        max_tokens    = <span class="nm">300</span>,
        stop          = [<span class="st">"}"</span>, <span class="st">"```"</span>],
        verbose       = <span class="kw">True</span>
    )

    <span class="fn">print</span>(<span class="st">"RESPONSE:\n"</span>, result[<span class="st">"response"</span>])
    <span class="fn">print</span>(<span class="st">f"Actual cost: ${result['actual_cost_usd']}"</span>)
    <span class="fn">print</span>(<span class="st">f"Tokens saved by compression: {result['tokens_saved']}"</span>)

    optimizer.<span class="fn">print_session_stats</span>()
</pre>
    </div>
  </section>

  <!-- HOW EACH LAYER WORKS -->
  <section>
    <div class="section-label">03 // Layer Breakdown</div>
    <h2>How Each Optimization Layer Works</h2>

    <h3>Layer 1 â€” Prompt Compression (Token Grouping Simulation)</h3>
    <p>This is the programmable equivalent of "subtoken grouping." Before your text hits the API, long repeated phrases are swapped for short aliases. The model never sees the long form â€” it only processes the alias, consuming far fewer tokens.</p>
    <div class="formula-box">
      "large language model" (4 tokens) â†’ "LLM" (1 token) = 75% saving on that phrase
    </div>

    <div class="step-row">
      <div class="step-num">1</div>
      <div class="step-body"><h4>Define your compression dictionary</h4><p>Map domain-specific repeated long phrases to short codes. Add as many as needed.</p></div>
    </div>
    <div class="step-row">
      <div class="step-num">2</div>
      <div class="step-body"><h4>Run _compress() on every message</h4><p>Regex replace happens client-side in milliseconds â€” zero latency penalty.</p></div>
    </div>
    <div class="step-row">
      <div class="step-num">3</div>
      <div class="step-body"><h4>Count saved tokens</h4><p>Use tiktoken to measure real token delta before and after compression.</p></div>
    </div>

    <h3>Layer 2 â€” Logit Bias (True Token Masking)</h3>
    <p>This is actual token-level masking in the model's output distribution. A logit bias of <span class="tag red">-100</span> effectively removes a token from the model's vocabulary for that call. We use this to ban filler words like "Certainly!", "Of course", etc. â€” these are pure wasted output tokens that add zero information value and cost you money.</p>

    <div class="code-block">
      <div class="code-title">How token IDs are found</div>
      <pre><span class="kw">import</span> tiktoken
enc = tiktoken.get_encoding(<span class="st">"cl100k_base"</span>)

<span class="cm"># Find the token ID for a word you want to ban</span>
word = <span class="st">"Certainly"</span>
ids  = enc.encode(word)
<span class="fn">print</span>(ids)  <span class="cm"># â†’ [34'Certa', 7..etc] â€” each sub-word is a token</span>

<span class="cm"># Apply in API call:</span>
logit_bias = { <span class="fn">str</span>(token_id): <span class="nm">-100</span> <span class="kw">for</span> token_id <span class="kw">in</span> ids }
<span class="cm"># This token will NEVER appear in the model's response</span></pre>
    </div>

    <h3>Layer 3 â€” Automatic Cache Exploitation</h3>
    <p>Kimi K2 automatically caches input tokens at <span class="tag green">$0.15/M</span> instead of <span class="tag red">$0.60/M</span> â€” a 75% reduction â€” when the same prefix is reused across calls. The trick: <strong>always put your long static system prompt FIRST</strong> so it becomes the cached prefix. Only the user message changes each call.</p>

    <h3>Layer 4 â€” Stop Sequences + max_tokens</h3>
    <p>Output tokens cost <strong>4Ã— more</strong> than input tokens. Stopping generation early is the highest-ROI single action. Use <code>stop=["###"]</code> to end structured output cleanly, and set <code>max_tokens</code> to the minimum needed for your task â€” not the default maximum.</p>
  </section>

  <!-- COST COMPARISON TABLE -->
  <section>
    <div class="section-label">04 // Cost Analysis</div>
    <h2>Before vs After: Real Cost Comparison</h2>
    <p>Scenario: A production chatbot making <strong>10,000 calls/month</strong>, each with a 1,200-token system prompt + 300-token user message + ~400-token response. This is a typical small-to-medium production workload.</p>

    <div class="cost-section">
      <div class="cost-header"><h3>// Token &amp; Cost Breakdown Per Call</h3></div>
      <table>
        <tr>
          <th>Metric</th>
          <th>âŒ No Optimization</th>
          <th>âœ… With Optimizer</th>
          <th>Saving</th>
        </tr>
        <tr>
          <td>System prompt tokens</td>
          <td class="cost-bad">1,200</td>
          <td class="cost-good">1,200 (cached)</td>
          <td class="savings">Billed at $0.15/M not $0.60/M</td>
        </tr>
        <tr>
          <td>User message tokens</td>
          <td class="cost-bad">300 tokens</td>
          <td class="cost-good">~240 tokens (compressed)</td>
          <td class="savings">~60 tokens saved (-20%)</td>
        </tr>
        <tr>
          <td>Output tokens</td>
          <td class="cost-bad">400 (uncapped)</td>
          <td class="cost-good">~300 (filler banned + capped)</td>
          <td class="savings">~100 tokens saved (-25%)</td>
        </tr>
        <tr>
          <td>Input cost per call</td>
          <td class="cost-bad">$0.000900</td>
          <td class="cost-good">$0.000396</td>
          <td class="savings">-56%</td>
        </tr>
        <tr>
          <td>Output cost per call</td>
          <td class="cost-bad">$0.001000</td>
          <td class="cost-good">$0.000750</td>
          <td class="savings">-25%</td>
        </tr>
        <tr>
          <td><strong>Total cost per call</strong></td>
          <td class="cost-bad"><strong>$0.001900</strong></td>
          <td class="cost-good"><strong>$0.001146</strong></td>
          <td class="savings"><strong>-39.7%</strong></td>
        </tr>
      </table>
    </div>

    <div class="cost-section">
      <div class="cost-header"><h3>// Monthly Cost at 10,000 Calls / Month</h3></div>
      <table>
        <tr>
          <th>Scale</th>
          <th>âŒ Unoptimized</th>
          <th>âœ… Optimized</th>
          <th>Monthly Saving</th>
          <th>Annual Saving</th>
        </tr>
        <tr>
          <td>10K calls/month (small)</td>
          <td class="cost-bad">$19.00</td>
          <td class="cost-good">$11.46</td>
          <td class="savings">$7.54</td>
          <td class="savings">$90.48</td>
        </tr>
        <tr>
          <td>100K calls/month (medium)</td>
          <td class="cost-bad">$190.00</td>
          <td class="cost-good">$114.60</td>
          <td class="savings">$75.40</td>
          <td class="savings">$904.80</td>
        </tr>
        <tr>
          <td>500K calls/month (production)</td>
          <td class="cost-bad">$950.00</td>
          <td class="cost-good">$573.00</td>
          <td class="savings">$377.00</td>
          <td class="savings">$4,524.00</td>
        </tr>
        <tr>
          <td>1M calls/month (high traffic)</td>
          <td class="cost-bad">$1,900.00</td>
          <td class="cost-good">$1,146.00</td>
          <td class="savings"><strong>$754.00</strong></td>
          <td class="savings"><strong>$9,048.00</strong></td>
        </tr>
      </table>
    </div>

    <div class="savings-banner">
      <div>
        <div class="savings-big">~40%</div>
        <div class="savings-label">average cost reduction per call</div>
      </div>
      <div class="savings-sep">|</div>
      <div>
        <div class="savings-big">$9K+</div>
        <div class="savings-label">annual savings at 1M calls/month</div>
      </div>
      <div class="savings-sep">|</div>
      <div>
        <div class="savings-big">75%</div>
        <div class="savings-label">input cost reduction via caching alone</div>
      </div>
    </div>
  </section>

  <!-- WHAT'S NOT POSSIBLE -->
  <section>
    <div class="section-label">05 // Hard Limits</div>
    <h2>What Is NOT Possible (No Matter What)</h2>

    <div class="verdict">
      <div class="verdict-box bad">
        <div class="verdict-title">âŒ IMPOSSIBLE â€” No API for This</div>
        <div class="verdict-item">Merge multiple tokens into a single new token at runtime</div>
        <div class="verdict-item">Access or override the internal BPE tokenizer</div>
        <div class="verdict-item">Send "compressed byte codes" instead of text</div>
        <div class="verdict-item">Control token probabilities mid-generation (streaming bias)</div>
        <div class="verdict-item">Pre-register custom vocabulary tokens without retraining</div>
      </div>
      <div class="verdict-box good">
        <div class="verdict-title">âœ… POSSIBLE â€” Covered in Code Above</div>
        <div class="verdict-item">Ban specific token IDs from output via logit_bias: -100</div>
        <div class="verdict-item">Boost preferred token IDs via logit_bias: +N</div>
        <div class="verdict-item">Simulate token grouping via client-side compression aliases</div>
        <div class="verdict-item">Exploit 75% cache discount on repeated static prompts</div>
        <div class="verdict-item">Hard-cap output length and use stop sequences</div>
      </div>
    </div>

    <p style="margin-top:16px;">The fundamental reason subtoken grouping is impossible client-side: the LLM's vocabulary is <strong>fixed at training time</strong>. You cannot inject new token groups at inference without retraining or fine-tuning the model. Client-side compression is the programmable workaround that achieves the same goal.</p>
  </section>

  <!-- QUICK INSTALL -->
  <section>
    <div class="section-label">06 // Quick Setup</div>
    <h2>Install &amp; Run in 3 Steps</h2>

    <div class="code-block">
      <div class="code-title">Terminal</div>
      <pre><span class="cm"># Step 1: Install dependencies</span>
pip install openai tiktoken

<span class="cm"># Step 2: Set your API key</span>
<span class="kw">export</span> KIMI_API_KEY=<span class="st">"sk-your-moonshot-key-here"</span>

<span class="cm"># Step 3: Run</span>
python kimi_optimizer.py</pre>
    </div>

    <p>Get your key at <strong>platform.moonshot.ai</strong> â†’ Console â†’ API Keys. Minimum $1 recharge to activate. First $5 recharge gets a $5 bonus voucher â€” doubles your budget on entry.</p>
  </section>

  <footer>
    <div>
      <span class="tag">Kimi K2 Â· $0.60/M input</span>
      <span class="tag">Cache hit Â· $0.15/M</span>
      <span class="tag">Output Â· $2.50/M</span>
      <span class="tag green">Est. savings Â· 40â€“85%</span>
    </div>
    <p style="margin-top:16px;">Research compiled Feb 2026 Â· Pricing from Moonshot AI official platform Â· All figures are estimates; actual savings depend on your workload and cache hit rate.</p>
  </footer>
</div>

</body>
</html>
